<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Tathagata Mitra | Senior Data Engineer</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="style.css">
</head>
<body>

<!-- HERO -->
<header class="hero">
    <img src="images/test-de-image.jpeg" alt="Tathagata Mitra">
    <h1>Tathagata Mitra</h1>
    <p class="subtitle">
        Senior Data Engineer building scalable, reliable data platforms across
        FinTech, Blockchain, and Cloud
    </p>
    <p class="location">Bengaluru, India</p>
</header>

<!-- ABOUT -->
<section>
    <h2>About Me</h2>
    <p>
        I am a Senior Data Engineer with 5+ years of experience designing and operating
        production-grade data platforms across FinTech, Insurance, and FMCG domains.
        My work focuses on building systems that are not only scalable and performant,
        but also cost-efficient, reliable, and easy to evolve.
    </p>
    <p>
        Currently at <strong>CoinDCX</strong>, India’s leading crypto exchange, I work on
        large-scale data pipelines, real-time streaming systems, and blockchain data
        integrations that power analytics, operations, and user-facing insights.
        I enjoy solving complex data problems where correctness, latency, and business
        impact matter.
    </p>
</section>

<!-- HOW I THINK -->
<section>
    <h2>How I Think About Data Engineering</h2>
    <ul>
        <li>Design data models and pipelines with long-term maintainability in mind.</li>
        <li>Choose batch vs streaming based on business criticality, not hype.</li>
        <li>Optimize for cost, reliability, and observability — not just performance.</li>
        <li>Build modular, testable pipelines using Delta Lake and Medallion Architecture.</li>
        <li>Treat data platforms as products, not one-time projects.</li>
    </ul>
</section>

<!-- EXPERIENCE -->
<section>
    <h2>Experience Highlights</h2>

    <h3>CoinDCX — Senior Software Engineer II (Data)</h3>
    <p class="duration">Nov 2022 – Present</p>
    <ul>
        <li>
            Built and scaled ETL pipelines using PySpark, SparkSQL, and Databricks on AWS S3,
            delivering clean, analysis-ready datasets for multiple business teams.
        </li>
        <li>
            Designed data marts and consolidated overlapping models, reducing I/O overhead
            and cutting model complexity by ~40%.
        </li>
        <li>
            Engineered real-time pipelines using Spark Structured Streaming, Kafka, and Delta Lake
            to monitor markets and generate rule-based event triggers.
        </li>
        <li>
            Built end-to-end blockchain data pipelines tracking holdings across Ethereum,
            Bitcoin, Tron, Solana, Polkadot, and other chains using Web3.py, REST APIs, and RPCs.
        </li>
        <li>
            Delivered cost-optimization initiatives, including an automated DeFi liquidation
            system saving ~USD 1,000/month and a custom archival platform reducing DB load by 40%.
        </li>
        <li>
            Developed GenAI-powered POCs including a trading indicator and an internal chatbot
            using OpenAI APIs, Azure services, and Python.
        </li>
        <li>
            Mentored junior engineers, led BAU initiatives, and interviewed 50+ data engineering
            candidates as part of hiring efforts.
        </li>
    </ul>

    <h3>Exusia — Senior Analyst (Data Engineering)</h3>
    <p class="duration">Apr 2022 – Oct 2022</p>
    <ul>
        <li>
            Built Spark pipelines in Scala on Azure Databricks to migrate on-prem data
            to Azure cloud platforms.
        </li>
        <li>
            Used Azure Data Factory and Blob Storage to orchestrate, cleanse, and optimize data flows.
        </li>
        <li>
            Implemented Delta Lake for performance improvements and storage optimization.
        </li>
    </ul>

    <h3>Cognizant — Programmer Analyst (Data Engineering)</h3>
    <p class="duration">Nov 2020 – Apr 2022</p>
    <ul>
        <li>
            Developed PySpark-based ingestion pipelines into Snowflake with Delta-based
            intermediate layers on AWS S3.
        </li>
        <li>
            Managed scheduling, monitoring, and CI/CD using Tidal, GitLab, and Jenkins.
        </li>
        <li>
            Performed data validation, testing, and production support for critical pipelines.
        </li>
    </ul>
</section>

<!-- SKILLS -->
<section>
    <h2>Skills</h2>

    <h3>Data Engineering & Processing</h3>
    <p>Python, SQL, PySpark, SparkSQL, Databricks, Delta Lake, ETL / ELT, Data Modeling</p>

    <h3>Streaming & Real-Time</h3>
    <p>Spark Structured Streaming, Kafka, Event-driven pipelines</p>

    <h3>Cloud & Platforms</h3>
    <p>AWS (S3, Lambda, DynamoDB), Azure (ADF, Functions, Blob Storage), Snowflake</p>

    <h3>Blockchain & AI</h3>
    <p>Web3.py, REST & RPC APIs, OpenAI / GPT-4, Azure AI Search, Streamlit</p>

    <h3>Engineering Practices</h3>
    <p>Cost Optimization, Observability, CI/CD, Agile, Mentoring, Hiring</p>
</section>

<!-- FOOTER -->
<footer>
    <p>
        © 2025 Tathagata Mitra ·
        <a href="https://www.linkedin.com/in/tathagata-mitra-187721186" target="_blank">LinkedIn</a>
    </p>
</footer>

</body>
</html>
